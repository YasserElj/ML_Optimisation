{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we focus on the minimization of general functions of the form: \n",
    "$$\\min_{x\\in\\mathbb{R}^n} f(x).$$\n",
    "\n",
    "the purpose of this practical session is to write descent algorithms, and evaluate performances, using the following function: \n",
    "<li>$f(x,y) = 100(y-x^2)^2 + (1-x)^2$ (Rosenbrock function).\n",
    "\n",
    "We denote $\\textit{oracle}$ a routine that for a given $x$, returns $f(x)$, the gradient $\\nabla f(x)$ (or approximation) it it exists, and eventually the Hessian $H[f](x)$ (or approximation) if it exists:\n",
    "\n",
    "$$[f(x),\\nabla f(x),H[f](x)] = \\textrm{oracle}(x)$$\n",
    "\n",
    "TODO: compute the gradient and Hessian for the rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** write here gradient and hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle(x):\n",
    "    try:\n",
    "        f = ## YOUR CODE HERE\n",
    "        df = np.zeros(2)\n",
    "        df[0] = ## YOUR CODE HERE\n",
    "        df[1] = ## YOUR CODE HERE\n",
    "        Hf = np.zeros((2,2))\n",
    "        Hf[0][0] = ## YOUR CODE HERE\n",
    "        ## ADD other entries for hessian\n",
    "        return f,df,Hf\n",
    "    except:\n",
    "        print(\"Could not compute function, gradient or hessian\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recall the GD process\n",
    "\n",
    "    Data : 𝑥0∈ℝ𝑛\n",
    "\n",
    "initial point and oracle.\n",
    "Init : iteration 𝑘=0\n",
    ".\n",
    "While convergence not reached, do\n",
    "\n",
    "    Compute descent direction 𝑑𝑘\n",
    "\n",
    ".\n",
    "Choose or compute learning rate 𝑠𝑘\n",
    ".\n",
    "Update 𝑥𝑘+1\n",
    ".\n",
    "𝑘=𝑘+1.\n",
    "\n",
    "    TO DO : analytic solution:\n",
    "\n",
    "Compute stationary points for rosenbrock, characterize the stationary point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**TODO :** compute mathematical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(function,h=0.1,xini=np.array([0.,0.])):\n",
    "    x = np.copy(xini)\n",
    "    y=[x]\n",
    "    eps = 1e-10\n",
    "    itermax = 10000\n",
    "    err = 2*eps\n",
    "    iter = 0\n",
    "    try:\n",
    "        while err>eps and iter<itermax:\n",
    "            f,df,Hf = function(x)\n",
    "            x = ## YOUR CODE HERE, gradient descent step\n",
    "            y.append(x)\n",
    "            err = np.linalg.norm(df)\n",
    "            iter += 1\n",
    "        xiter=np.array(y)\n",
    "    except:\n",
    "        print(\"Could not converge\")\n",
    "    return x,xiter,iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(function,xini=[0,0]):\n",
    "    x = np.copy(xini)\n",
    "    y=[x]\n",
    "    eps = 1e-10\n",
    "    itermax = 1000\n",
    "    err = 2*eps\n",
    "    iter = 0\n",
    "    while err>eps and iter<itermax:      \n",
    "        f,df,Hf = function(x)     \n",
    "        x = ## YOUR CODE HERE, second order gradient descent step\n",
    "        y.append(x)\n",
    "        err = np.linalg.norm(df)\n",
    "        iter += 1\n",
    "    xiter=np.array(y)\n",
    "    return x,xiter,iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Visualization :** Understand the following routine that plots level lines of the function, as well as the iterative minimization points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = 1000\n",
    "Ny = 1000\n",
    "x = np.linspace(0.5,2,Nx)\n",
    "y = np.linspace(0.5,2,Ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = 100*(X+Y-2)**2+(X-Y)**2\n",
    "CS=plt.contour(X, Y, Z,[0,0.1,1,5,12,24],colors='b')\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "z = [[1,1.5],[0,0.5]]\n",
    "plt.scatter(z[0],z[1],marker='o')\n",
    "plt.show()\n",
    "def affichage(xiter) :\n",
    "    Nx = 1000\n",
    "    Ny = 1000\n",
    "    x = np.linspace(0.5,1.5,Nx)\n",
    "    y = np.linspace(0.5,1.5,Ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = 100*(X+Y-2)**2+(X-Y)**2\n",
    "    CS=plt.contour(X, Y, Z,[0,0.1,2,5,10],colors='k')\n",
    "    plt.scatter(xiter[:,0], xiter[:,1],marker='o')\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "affichage(np.array(z))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: test first order, second order descent methods with various init points and step sizes, and conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convergence usuelle\n",
    "x,xiter,t=Gradient(oracle)\n",
    "affichage(xiter)\n",
    "plt.show()\n",
    "print(\"a : convergence vers \",x,\" nb iterations : \",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test2 : pour faire converger la descente, on prend des pas minuscules\n",
    "x,xiter,t=Gradient(oracle,h=2e-3)\n",
    "affichage(xiter)\n",
    "plt.show()\n",
    "print(\"b : convergence vers \",x,\" nb iterations : \",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test3 : pour faire converger le test1, on se rapproche du min\n",
    "# Cela ne fonctionne toujours pas, le pas est trop gros\n",
    "x,xiter,t=Gradient(oracle,h=2e-3,xini=[1.1,1.1])\n",
    "affichage(xiter)\n",
    "plt.show()\n",
    "print(\"b : convergence vers \",x,\" nb iterations : \",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAnother funny case\n",
    "x,xiter,t=Gradient(oracle,xini=[1,2],h=2e-3)\n",
    "affichage(xiter)\n",
    "plt.show()\n",
    "print(\"d : convergence vers \",x,\" nb iterations : \",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test4 : convergence pour Newton\n",
    "x,xiter,t=Newton(oracle)\n",
    "affichage(xiter)\n",
    "plt.show()\n",
    "print( \"e : convergence vers \",x,\" nb iterations : \",t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test1 : Newton, on s'éloigne un peu du min, cela fonctionne\n",
    "x,xiter,t=Newton(oracle,xini=[10,10])\n",
    "affichage(xiter)\n",
    "plt.show()\n",
    "print( \"f : convergence vers \",x,\" nb iterations : \",t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus, implement heavy-ball method\n",
    "where $x_{t+1} = x_t -\\alpha \\nabla f + \\beta (x_t-x_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second bonus:\n",
    "Change function to $f(x,y) = (y-x^2)^2 + (1-x)^2$. Do you observe better convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume a hypothetical experiment in which students of agriculture science collected some data from different farms/green houses through out many years. What we want to do is predict harvests $y$ of different farms/green houses given Average Temperature $x_1$ and Average Nitrite In Soil $x_2$. For this we have made some assumptions based on initial analysis of the data.\n",
    "\n",
    "Let us assume a simple linear model $y = w_0 + w_1 x_1 + w_2 x_2$\n",
    "\n",
    "We will here use a gradient descent method to solve this linear system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of data points\n",
    "nb_pts=100\n",
    "\n",
    "# np.random.randn(100) will give array of 100 normally distributed random \n",
    "# numbers with mean 0 and std-dev 1\n",
    "\n",
    "# 2*np.random.randn(100)+12 changes the normal distrubution to have mean 12 and std-dev 2\n",
    "nitrate = 2*np.random.randn(nb_pts)+12\n",
    "temperature = 4*np.random.rand(nb_pts) + 26\n",
    "\n",
    "# np.c_ concatinates (joins) two array column wise\n",
    "x_farm = np.c_[nitrate,temperature]\n",
    "\n",
    "# This is imaginary equation describing relation between yeild, nitrate and temperature.\n",
    "# Obiously this is not know in real world problem. We are using it to generate dummy data.\n",
    "yeild_ideal = .1*nitrate + .08*temperature +.6\n",
    "\n",
    "# adding some noise on the ideal equation. \n",
    "# The noise is normally distributed with 0 mean and std-dev 0.4\n",
    "yeild = yeild_ideal + .4*np.random.randn(nb_pts)\n",
    "\n",
    "print(\"few instances of generated data \\n\", x_farm[:5])\n",
    "print(\"few instances of generated targets \\n\", yeild[:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bais_term(x):\n",
    "    \n",
    "    # np.ones(n) will give new array of length n whose all elements are 1 \n",
    "    # np.c_ concatinates two array column wise\n",
    "    return np.c_[np.ones(len(x)),x]\n",
    "\n",
    "\n",
    "# Root mean square cost function\n",
    "def rmse_cost_func(P,Y):\n",
    "    ## model is array with bais and coffecients values\n",
    "    rmse = ## YOUR CODE HERE\n",
    "    return rmse\n",
    "\n",
    "\n",
    "# Finds gradient of cost function using eq(5.2) above\n",
    "def gradient_of_cost(x,y,model):\n",
    "    preds = predict(x,model)\n",
    "    \n",
    "    error_term = preds-y\n",
    "    \n",
    "    # np.matmul performs matrix multiplication\n",
    "    # x.T is transpose of matrix x\n",
    "    xt_dot_error_term = ## YOUR CODE HERE, gradient of linear regression method\n",
    "    return xt_dot_error_term\n",
    "\n",
    "# Do prediction using eq(5.1) above\n",
    "def predict(x,model):\n",
    "    #np.matmul performs matrix multiplication\n",
    "    return np.matmul(x,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimum values of bais and coefficient using gradient descent\n",
    "def find_linear_regression_model(x,y):\n",
    "    n_epochs = 1000000\n",
    "    eps = 0.1\n",
    "    neta = 0.001\n",
    "    err = 2*eps\n",
    "    iter=0\n",
    "    \n",
    "    # Initialize all parameters(wj's) to zero\n",
    "    model = np.zeros(len(x[0]))\n",
    "    \n",
    "    # do n_epochs iteration\n",
    "    while err>eps and iter<n_epochs:\n",
    "        # compute gradient\n",
    "        grad = gradient_of_cost(x,y,model)\n",
    "        # compute error\n",
    "        err=rmse_cost_func(predict(x,model),y)\n",
    "        # move parameters closer to optimum solution in every step \n",
    "        next_model = model - neta*grad\n",
    "        model = next_model\n",
    "        iter += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bias term\n",
    "x_farm_with_bais = add_bais_term(x_farm)\n",
    "# compute model\n",
    "model = find_linear_regression_model(x_farm_with_bais, yeild)\n",
    "print(\"\\nmodel(w0,w1,w2)\\n\", model)\n",
    "# ideal model: 0.6 + .1*nitrate + .08*temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Gradient descent method for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly_express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode\n",
    "from plotly.offline import plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set notebook mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data using panda frame\n",
    "import os\n",
    "print(os.getcwd())\n",
    "df = pd.read_csv(\"../TP-Regression/possum.csv\")\n",
    "df = df.drop('case', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = df.shape[0]\n",
    "# extract x values\n",
    "X=df['chest'].to_numpy().reshape(n_rows)\n",
    "#extract some Y data\n",
    "y=df['belly'].to_numpy().reshape(n_rows,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the input data\n",
    "plt.scatter(X,y)\n",
    "plt.xlabel('X'); plt.ylabel('y');\n",
    "plt.title('Input dataset');\n",
    "z = np.linspace(np.min(X),np.max(X))\n",
    "plt.plot(z, z+5, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.c_[X,np.ones(len(X))]\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta=np.array([[0],[0]])):\n",
    "    \"\"\"Given covariate matrix X, the prediction results y and coefficients theta\n",
    "    compute the loss\"\"\"\n",
    "    \n",
    "    m = len(y)\n",
    "    J=0 # initialize loss to zero\n",
    "    \n",
    "    # reshape theta\n",
    "    theta=theta.reshape(2,1)\n",
    "    \n",
    "    # calculate the hypothesis - y_hat\n",
    "    h_x = np.dot(X,theta)\n",
    "    \n",
    "    # subtract y from y_hat, square and sum\n",
    "    error_term = sum((h_x - y)**2)\n",
    "    \n",
    "    # divide by twice the number of samples - standard practice.\n",
    "    loss = error_term/(2*m)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(A,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta=np.array([[0],[0]]),\n",
    "                    alpha=0.01, num_iterations=1500):\n",
    "    \"\"\"\n",
    "    Solve for theta using Gradient Descent optimiztion technique. \n",
    "    Alpha is the learning rate\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "    theta0_history = []\n",
    "    theta1_history = []\n",
    "    theta = theta.reshape(2,1)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        error = (np.dot(X, theta) - y)\n",
    "        \n",
    "        term0 = (alpha/m) * sum(error* X[:,0].reshape(m,1))\n",
    "        term1 = (alpha/m) * sum(error* X[:,1].reshape(m,1))\n",
    "        \n",
    "        # update theta\n",
    "        term_vector = np.array([[term0],[term1]])\n",
    "#         print(term_vector)\n",
    "        theta = theta - term_vector.reshape(2,1)\n",
    "        \n",
    "        # store history values\n",
    "        theta0_history.append(theta[0].tolist()[0])\n",
    "        theta1_history.append(theta[1].tolist()[0])\n",
    "        J_history.append(compute_cost(X,y,theta).tolist()[0])\n",
    "        \n",
    "    return (theta, J_history, theta0_history, theta1_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_iterations=50000\n",
    "theta_init=np.array([[1],[3]])\n",
    "alpha=0.001\n",
    "theta, J_history, theta0_history, theta1_history = gradient_descent(A,y, theta_init,\n",
    "                                                                   alpha, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# plot thetas over time\n",
    "color='tab:blue'\n",
    "ax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color)\n",
    "ax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color)\n",
    "# ax1.legend()\n",
    "ax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color);\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# plot loss function over time\n",
    "color='tab:red'\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(J_history, label='Loss function', color=color)\n",
    "ax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations')\n",
    "ax2.set_ylabel('Loss: $J(\\\\theta)$', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# ax2.legend();\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the input data\n",
    "plt.scatter(X,y)\n",
    "plt.xlabel('X'); plt.ylabel('y');\n",
    "plt.title('Input dataset');\n",
    "z = np.linspace(np.min(X),np.max(X))\n",
    "plt.plot(z, theta[0]*z+theta[1], 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we compare the results of GD, with direct solving of the mean-square problem, what can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
