{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the rosenbrock function $f(x,y) = 100(y-x^2)^2 + (1-x)^2$ \n",
    "\n",
    "We will in this practical session see what automatoc differentiation is with pytorch, and how to optimize with pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrok(x):\n",
    "    ## YOUR CODE HERE\n",
    "    f = \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([.5,.5], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "n_iter = 10000\n",
    "lr = torch.tensor(0.001, dtype=torch.float64)\n",
    "\n",
    "\n",
    "# define optimizer \n",
    "optimizer = torch.optim.SGD([x], lr=lr)\n",
    "\n",
    "for i in range(1, n_iter):\n",
    "    # zero out the gradient (by default PyTorch accumulates gradients)\n",
    "    optimizer.zero_grad()\n",
    "    y = rosenbrok(x)\n",
    "    # compute the gradient\n",
    "    y.backward(retain_graph=True)\n",
    "    # apply GD step\n",
    "    optimizer.step()\n",
    "print(\"Final value: \",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "try different initial conditions and learning rate\n",
    "see if schema is more robust, change optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation in pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "z=x**2 + torch.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Value of z: \",z)\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx=x.grad\n",
    "dy=y.grad\n",
    "print(\"Gradient w.r.t. x : \",dx)\n",
    "print(\"Gradient w.r.t. y : \",dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: plugin the computational graph studied in course, verify the manual computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classification network with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "#importing Libraries\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_count = df['Outcome'].value_counts()\n",
    "plt.figure(figsize=(7,5))\n",
    "#sns.barplot(patient_count.index, patient_count.values, alpha=0.8 , palette = 'rocket')\n",
    "# graph here barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of glucose features\n",
    "sns.distplot(df['Glucose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of body mass index\n",
    "sns.distplot(df['BMI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of blood pressure\n",
    "sns.distplot(df['BloodPressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and outcome\n",
    "X = df.drop('Outcome' , axis = 1) #independent Feature\n",
    "y = df['Outcome'] #dependent Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit learn to easily split data into two sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y , test_size =0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tensors\n",
    "X_train=torch.FloatTensor(X_train.values)\n",
    "X_test=torch.FloatTensor(X_test.values)\n",
    "y_train=torch.LongTensor(y_train.values)\n",
    "y_test=torch.LongTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dimension of the entry\n",
    "input_dim=X_train.shape[1]\n",
    "print(\"Number of input features:\",input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Model\n",
    "class ANN_model(nn.Module):\n",
    "    def __init__(self,input_features=input_dim,hidden1=20, hidden2=10,out_features=2):\n",
    "        super().__init__()\n",
    "        self.f_connected1 = nn.Linear(input_features,hidden1)\n",
    "        self.f_connected2 = nn.Linear(hidden1,hidden2)\n",
    "        self.out = nn.Linear(hidden2,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.f_connected1(x))\n",
    "        x = F.relu(self.f_connected2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare an instance of the model and display parameters\n",
    "#torch.manual_seed(20)\n",
    "model = ANN_model()\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare loss function to use, as well as optimizer\n",
    "# Backward Propergation - loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5000\n",
    "final_losses=[]\n",
    "for i in range(epochs):\n",
    "    i= i+1\n",
    "    y_pred=model.forward(X_train)\n",
    "    loss=loss_function(y_pred,y_train)\n",
    "    final_losses.append(loss.item())\n",
    "    if i % 100 == 0:\n",
    "        print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss. # YOUR CODE HERE\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the loss function\n",
    "plt.plot(range(epochs),final_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.ylabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_test):\n",
    "        y_pred = # YOUR CODE HERE\n",
    "        predictions.append(y_pred.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "cm\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- try to improve and play with different parameters (optimizer, hyperparameters, network structure, etc)\n",
    "- train a classifier on the customer dataset (seen during practical session logistic): can we improve over logistic regression?\n",
    "- train a NN for prediction, compare with results obtained during last sessions (TP-Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn MNIST classifier (lecun net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch,torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly_express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode\n",
    "from plotly.offline import plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        # input is 28x28\n",
    "        # padding=2 for same padding\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        # feature map size is 14*14 by pooling\n",
    "        # padding=2 for same padding\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        # feature map size is 7*7 by pooling\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*7*7)   # reshape Variable\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "model = MnistModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "i = 0\n",
    "for epoch in range(1):\n",
    "    for data, target in train_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #loss = F.cross_entropy(output, target)\n",
    "        loss.backward()    # calc gradients\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.step()   # update gradients\n",
    "        prediction = output.data.max(1)[1]   # first column has actual prob.\n",
    "        accuracy = prediction.eq(target.data).sum()/batch_size*100\n",
    "        train_accu.append(accuracy)\n",
    "        if i % 100 == 0:\n",
    "            print('Train Step: {}\\tLoss: {:.3f}\\tAccuracy: {:.3f}'.format(i, loss.item(), accuracy))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(np.arange(len(train_loss)), train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(np.arange(len(train_accu)), train_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    prediction = output.data.max(1)[1]\n",
    "    correct += prediction.eq(target.data).sum()\n",
    "\n",
    "print('\\nTest set: Accuracy: {:.2f}%'.format(100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
